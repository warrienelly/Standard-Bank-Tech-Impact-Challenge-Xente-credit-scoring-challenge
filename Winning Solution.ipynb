{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np  \n",
    "import seaborn as sns\n",
    "import pandas as pd, os, gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleValidationEncoderNumerical:\n",
    "    \"\"\"\n",
    "    Encoder with validation within\n",
    "    \"\"\"\n",
    "    def __init__(self, cols: List, encoder, folds):\n",
    "        \"\"\"\n",
    "        :param cols: Categorical columns\n",
    "        :param encoder: Encoder class\n",
    "        :param folds: Folds to split the data\n",
    "        \"\"\"\n",
    "        self.cols = cols\n",
    "        self.encoder = encoder\n",
    "        self.encoders_dict = {}\n",
    "        self.folds = folds\n",
    "\n",
    "    def fit_transform(self, X: pd.DataFrame, y: np.array) -> pd.DataFrame:\n",
    "        X = X.reset_index(drop=True)\n",
    "        y = y.reset_index(drop=True)\n",
    "        for n_fold, (train_idx, val_idx) in enumerate(self.folds.split(X, y)):\n",
    "            X_train, X_val = X.loc[train_idx].reset_index(drop=True), X.loc[val_idx].reset_index(drop=True)\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            _ = self.encoder.fit_transform(X_train, y_train)\n",
    "\n",
    "            # transform validation part and get all necessary cols\n",
    "            val_t = self.encoder.transform(X_val)\n",
    "\n",
    "            if n_fold == 0:\n",
    "                cols_representation = np.zeros((X.shape[0], val_t.shape[1]))\n",
    "            \n",
    "            self.encoders_dict[n_fold] = self.encoder\n",
    "\n",
    "            cols_representation[val_idx, :] += val_t.values\n",
    "\n",
    "        cols_representation = pd.DataFrame(cols_representation, columns=X.columns)\n",
    "\n",
    "        return cols_representation\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = X.reset_index(drop=True)\n",
    "\n",
    "        cols_representation = None\n",
    "\n",
    "        for encoder in self.encoders_dict.values():\n",
    "            test_tr = encoder.transform(X)\n",
    "\n",
    "            if cols_representation is None:\n",
    "                cols_representation = np.zeros(test_tr.shape)\n",
    "\n",
    "            cols_representation = cols_representation + test_tr / self.folds.n_splits\n",
    "\n",
    "        cols_representation = pd.DataFrame(cols_representation, columns=X.columns)\n",
    "        \n",
    "        return cols_representation\n",
    "\n",
    "\n",
    "class FrequencyEncoder:\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "        self.counts_dict = None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame, y=None) -> pd.DataFrame:\n",
    "        counts_dict = {}\n",
    "        for col in self.cols:\n",
    "            values, counts = np.unique(X[col], return_counts=True)\n",
    "            counts_dict[col] = dict(zip(values, counts))\n",
    "        self.counts_dict = counts_dict\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        counts_dict_test = {}\n",
    "        res = []\n",
    "        for col in self.cols:\n",
    "            values, counts = np.unique(X[col], return_counts=True)\n",
    "            counts_dict_test[col] = dict(zip(values, counts))\n",
    "\n",
    "            # if value is in \"train\" keys - replace \"test\" counts with \"train\" counts\n",
    "            for k in [key for key in counts_dict_test[col].keys() if key in self.counts_dict[col].keys()]:\n",
    "                counts_dict_test[col][k] = self.counts_dict[col][k]\n",
    "\n",
    "            res.append(X[col].map(counts_dict_test[col]).values.reshape(-1, 1))\n",
    "        res = np.hstack(res)\n",
    "\n",
    "        X[self.cols] = res\n",
    "        return X\n",
    "\n",
    "    def fit_transform(self, X: pd.DataFrame, y=None) -> pd.DataFrame:\n",
    "        self.fit(X, y)\n",
    "        X = self.transform(X)\n",
    "        return X\n",
    "\n",
    "## Reduce memory usage\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parsing Date columns\n",
    "train = pd.read_csv('C:/Users/Bless/Downloads/Data Science/Competitions/Standard Bank Tech Impact Challenge Xente credit scoring challenge/Train.csv',parse_dates=['TransactionStartTime','PaidOnDate','DueDate','IssuedDateLoan'])\n",
    "test = pd.read_csv('C:/Users/Bless/Downloads/Data Science/Competitions/Standard Bank Tech Impact Challenge Xente credit scoring challenge/Test.csv',parse_dates=['TransactionStartTime','IssuedDateLoan'])\n",
    "sample =  pd.read_csv('C:/Users/Bless/Downloads/Data Science/Competitions/Standard Bank Tech Impact Challenge Xente credit scoring challenge/sample_submission.csv')\n",
    "unlinked_masked_final = pd.read_csv('C:/Users/Bless/Downloads/Data Science/Competitions/Standard Bank Tech Impact Challenge Xente credit scoring challenge/unlinked_masked_final.csv',parse_dates=['TransactionStartTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2100, 27), (905, 19), (16327, 12))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape, unlinked_masked_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CustomerId', 'TransactionStartTime', 'Value', 'Amount',\n",
       "       'TransactionId', 'BatchId', 'SubscriptionId', 'CurrencyCode',\n",
       "       'CountryCode', 'ProviderId', 'ProductId', 'ProductCategory',\n",
       "       'ChannelId', 'TransactionStatus', 'IssuedDateLoan', 'AmountLoan',\n",
       "       'Currency', 'LoanId', 'PaidOnDate', 'IsFinalPayBack', 'InvestorId',\n",
       "       'DueDate', 'LoanApplicationId', 'PayBackId', 'ThirdPartyId',\n",
       "       'IsThirdPartyConfirmed', 'IsDefaulted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get cat and Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col = train.select_dtypes(include=np.number).columns\n",
    "cat_col = train.select_dtypes(exclude=np.number).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduce Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  0.35 Mb (19.9% reduction)\n",
      "Mem. usage decreased to  0.11 Mb (13.8% reduction)\n"
     ]
    }
   ],
   "source": [
    "train = reduce_mem_usage(train)\n",
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Missing values Columns\n",
    "This is done inorder to check how to impute missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomerId                 0\n",
       "TransactionStartTime       0\n",
       "Value                      0\n",
       "Amount                     0\n",
       "TransactionId              0\n",
       "BatchId                    0\n",
       "SubscriptionId             0\n",
       "CurrencyCode               0\n",
       "CountryCode                0\n",
       "ProviderId                 0\n",
       "ProductId                  0\n",
       "ProductCategory            0\n",
       "ChannelId                  0\n",
       "TransactionStatus          0\n",
       "IssuedDateLoan           612\n",
       "AmountLoan               612\n",
       "Currency                 612\n",
       "LoanId                   612\n",
       "PaidOnDate               612\n",
       "IsFinalPayBack           612\n",
       "InvestorId               612\n",
       "DueDate                  614\n",
       "LoanApplicationId        617\n",
       "PayBackId                612\n",
       "ThirdPartyId             614\n",
       "IsThirdPartyConfirmed    612\n",
       "IsDefaulted              612\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After findings, missing values was as a result of rejected loan or no loan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransactionStartTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No transaction has a multiple Default state, its either a transaction was paid totally or defaulted totally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those with missing values did not take any loan and as such, fill the values with -999\n",
    "or drop them.\n",
    "\n",
    "I later resolved to drop the, since they have no default state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fill Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "# train.fillna(0)\n",
    "# train.fillna(-999)\n",
    "# test.fillna(-999)\n",
    "# train.fillna(train.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## TO be tried later\n",
    "# # T-SNE Implementation\n",
    "# X_reduced_tsne = TSNE(n_components=2, random_state=1996).fit_transform(X.values)\n",
    "\n",
    "# # PCA Implementation\n",
    "# X_reduced_pca = PCA(n_components=2, random_state=1996).fit_transform(X.values)\n",
    "\n",
    "# # TruncatedSVD\n",
    "# X_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check values in train and not in test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ChannelId', 'Currency', 'CurrencyCode', 'ProviderId', 'InvestorId',\n",
       "       'SubscriptionId', 'ProductCategory', 'ProductId', 'CustomerId',\n",
       "       'DueDate', 'LoanApplicationId', 'IssuedDateLoan', 'LoanId',\n",
       "       'ThirdPartyId', 'PaidOnDate', 'PayBackId', 'BatchId', 'TransactionId',\n",
       "       'TransactionStartTime'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[cat_col].nunique().sort_values().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not in Test ChannelId  []\n",
      "Not in Train ChannelId  []\n",
      "====================================\n",
      "Not in Test ProviderId  []\n",
      "Not in Train ProviderId  []\n",
      "====================================\n",
      "Not in Test InvestorId  [nan, 'InvestorId_3']\n",
      "Not in Train InvestorId  [nan]\n",
      "====================================\n",
      "Not in Test SubscriptionId  ['SubscriptionId_2', 'SubscriptionId_4', 'SubscriptionId_6']\n",
      "Not in Train SubscriptionId  ['SubscriptionId_3']\n",
      "====================================\n",
      "Not in Test ProductCategory  []\n",
      "Not in Train ProductCategory  ['ticket']\n",
      "====================================\n",
      "Not in Test ProductId  ['ProductId_16']\n",
      "Not in Train ProductId  ['ProductId_14', 'ProductId_12', 'ProductId_11']\n",
      "====================================\n",
      "Not in Test CustomerId  ['CustomerId_305', 'CustomerId_433', 'CustomerId_329', 'CustomerId_405', 'CustomerId_266', 'CustomerId_303', 'CustomerId_144', 'CustomerId_136', 'CustomerId_492', 'CustomerId_125', 'CustomerId_71', 'CustomerId_431', 'CustomerId_1', 'CustomerId_425', 'CustomerId_249', 'CustomerId_493', 'CustomerId_119', 'CustomerId_31', 'CustomerId_453', 'CustomerId_351', 'CustomerId_114', 'CustomerId_357', 'CustomerId_501', 'CustomerId_339', 'CustomerId_82', 'CustomerId_255', 'CustomerId_436', 'CustomerId_484', 'CustomerId_429', 'CustomerId_318', 'CustomerId_396', 'CustomerId_350', 'CustomerId_434', 'CustomerId_439', 'CustomerId_468', 'CustomerId_383', 'CustomerId_271', 'CustomerId_471', 'CustomerId_294', 'CustomerId_234', 'CustomerId_491', 'CustomerId_239', 'CustomerId_438', 'CustomerId_499', 'CustomerId_35', 'CustomerId_328', 'CustomerId_314', 'CustomerId_435', 'CustomerId_291', 'CustomerId_202', 'CustomerId_160', 'CustomerId_298', 'CustomerId_97', 'CustomerId_241', 'CustomerId_48', 'CustomerId_385', 'CustomerId_465', 'CustomerId_470', 'CustomerId_115', 'CustomerId_452', 'CustomerId_229', 'CustomerId_404', 'CustomerId_217', 'CustomerId_358', 'CustomerId_187', 'CustomerId_166', 'CustomerId_26', 'CustomerId_345', 'CustomerId_228', 'CustomerId_503', 'CustomerId_409', 'CustomerId_221', 'CustomerId_319', 'CustomerId_363', 'CustomerId_391', 'CustomerId_246', 'CustomerId_460', 'CustomerId_69', 'CustomerId_411', 'CustomerId_324', 'CustomerId_54', 'CustomerId_347', 'CustomerId_317', 'CustomerId_444', 'CustomerId_132', 'CustomerId_2', 'CustomerId_302', 'CustomerId_9', 'CustomerId_462', 'CustomerId_278', 'CustomerId_103', 'CustomerId_402', 'CustomerId_489', 'CustomerId_440', 'CustomerId_445', 'CustomerId_171', 'CustomerId_147', 'CustomerId_306', 'CustomerId_322', 'CustomerId_430', 'CustomerId_109', 'CustomerId_451', 'CustomerId_257', 'CustomerId_364', 'CustomerId_237', 'CustomerId_349', 'CustomerId_164', 'CustomerId_369', 'CustomerId_162', 'CustomerId_13', 'CustomerId_38', 'CustomerId_394', 'CustomerId_198', 'CustomerId_37', 'CustomerId_342', 'CustomerId_422', 'CustomerId_478', 'CustomerId_494', 'CustomerId_62', 'CustomerId_498', 'CustomerId_281', 'CustomerId_380', 'CustomerId_235', 'CustomerId_395', 'CustomerId_287', 'CustomerId_106', 'CustomerId_20', 'CustomerId_321', 'CustomerId_397', 'CustomerId_73', 'CustomerId_46', 'CustomerId_272', 'CustomerId_354', 'CustomerId_424', 'CustomerId_374', 'CustomerId_222', 'CustomerId_476', 'CustomerId_367', 'CustomerId_176', 'CustomerId_312', 'CustomerId_93', 'CustomerId_61', 'CustomerId_378', 'CustomerId_467', 'CustomerId_126', 'CustomerId_384', 'CustomerId_170', 'CustomerId_124', 'CustomerId_224', 'CustomerId_12', 'CustomerId_392', 'CustomerId_332', 'CustomerId_118', 'CustomerId_215', 'CustomerId_40', 'CustomerId_216', 'CustomerId_223', 'CustomerId_30', 'CustomerId_477', 'CustomerId_382', 'CustomerId_90', 'CustomerId_18', 'CustomerId_178', 'CustomerId_42', 'CustomerId_56', 'CustomerId_94', 'CustomerId_65', 'CustomerId_175', 'CustomerId_11', 'CustomerId_87', 'CustomerId_84', 'CustomerId_177', 'CustomerId_74', 'CustomerId_41', 'CustomerId_112', 'CustomerId_173', 'CustomerId_141', 'CustomerId_475', 'CustomerId_231', 'CustomerId_232', 'CustomerId_204', 'CustomerId_490', 'CustomerId_149', 'CustomerId_137', 'CustomerId_113', 'CustomerId_161', 'CustomerId_227', 'CustomerId_68', 'CustomerId_427', 'CustomerId_78', 'CustomerId_192', 'CustomerId_486', 'CustomerId_139', 'CustomerId_4', 'CustomerId_417', 'CustomerId_95', 'CustomerId_183', 'CustomerId_366', 'CustomerId_191', 'CustomerId_88', 'CustomerId_70', 'CustomerId_206', 'CustomerId_220', 'CustomerId_15', 'CustomerId_8', 'CustomerId_50', 'CustomerId_134', 'CustomerId_226', 'CustomerId_479', 'CustomerId_301', 'CustomerId_299', 'CustomerId_45', 'CustomerId_195', 'CustomerId_10', 'CustomerId_214', 'CustomerId_365', 'CustomerId_244', 'CustomerId_6', 'CustomerId_131', 'CustomerId_76', 'CustomerId_289', 'CustomerId_207', 'CustomerId_80', 'CustomerId_472', 'CustomerId_163', 'CustomerId_181', 'CustomerId_443', 'CustomerId_146', 'CustomerId_51', 'CustomerId_209', 'CustomerId_194', 'CustomerId_64', 'CustomerId_448', 'CustomerId_143', 'CustomerId_174', 'CustomerId_3', 'CustomerId_449', 'CustomerId_196', 'CustomerId_338', 'CustomerId_7', 'CustomerId_157', 'CustomerId_210', 'CustomerId_152', 'CustomerId_25', 'CustomerId_182', 'CustomerId_401', 'CustomerId_111', 'CustomerId_441', 'CustomerId_389', 'CustomerId_22', 'CustomerId_371', 'CustomerId_240', 'CustomerId_86', 'CustomerId_201', 'CustomerId_208', 'CustomerId_410', 'CustomerId_352', 'CustomerId_418', 'CustomerId_230', 'CustomerId_123', 'CustomerId_116', 'CustomerId_167', 'CustomerId_159', 'CustomerId_284', 'CustomerId_100', 'CustomerId_5', 'CustomerId_268', 'CustomerId_238', 'CustomerId_340', 'CustomerId_442', 'CustomerId_450', 'CustomerId_263', 'CustomerId_165', 'CustomerId_150', 'CustomerId_218', 'CustomerId_189', 'CustomerId_117', 'CustomerId_32', 'CustomerId_184', 'CustomerId_307', 'CustomerId_29', 'CustomerId_81', 'CustomerId_286', 'CustomerId_17', 'CustomerId_190', 'CustomerId_172', 'CustomerId_36', 'CustomerId_454', 'CustomerId_500', 'CustomerId_105', 'CustomerId_14', 'CustomerId_252', 'CustomerId_154', 'CustomerId_101', 'CustomerId_128', 'CustomerId_156', 'CustomerId_33', 'CustomerId_140', 'CustomerId_275', 'CustomerId_153', 'CustomerId_455', 'CustomerId_245', 'CustomerId_420', 'CustomerId_43', 'CustomerId_169', 'CustomerId_92', 'CustomerId_59', 'CustomerId_225', 'CustomerId_360', 'CustomerId_91', 'CustomerId_203', 'CustomerId_102', 'CustomerId_63', 'CustomerId_19', 'CustomerId_414', 'CustomerId_158', 'CustomerId_53', 'CustomerId_341', 'CustomerId_495', 'CustomerId_24', 'CustomerId_400', 'CustomerId_211', 'CustomerId_219', 'CustomerId_49', 'CustomerId_387', 'CustomerId_133', 'CustomerId_335', 'CustomerId_421', 'CustomerId_16', 'CustomerId_205']\n",
      "Not in Train CustomerId  ['CustomerId_107', 'CustomerId_250', 'CustomerId_293', 'CustomerId_251', 'CustomerId_77', 'CustomerId_464', 'CustomerId_262', 'CustomerId_373', 'CustomerId_426', 'CustomerId_377', 'CustomerId_333', 'CustomerId_386', 'CustomerId_21', 'CustomerId_23', 'CustomerId_504', 'CustomerId_483', 'CustomerId_96', 'CustomerId_236', 'CustomerId_487', 'CustomerId_416', 'CustomerId_412', 'CustomerId_337', 'CustomerId_375', 'CustomerId_456', 'CustomerId_300', 'CustomerId_186', 'CustomerId_121', 'CustomerId_138', 'CustomerId_463', 'CustomerId_413', 'CustomerId_331', 'CustomerId_485', 'CustomerId_469', 'CustomerId_481', 'CustomerId_327', 'CustomerId_52', 'CustomerId_264', 'CustomerId_148', 'CustomerId_199', 'CustomerId_60', 'CustomerId_260', 'CustomerId_408', 'CustomerId_248', 'CustomerId_457', 'CustomerId_482', 'CustomerId_309', 'CustomerId_497', 'CustomerId_466', 'CustomerId_104', 'CustomerId_459', 'CustomerId_480', 'CustomerId_353', 'CustomerId_428', 'CustomerId_403', 'CustomerId_270', 'CustomerId_330', 'CustomerId_355', 'CustomerId_447', 'CustomerId_290', 'CustomerId_85', 'CustomerId_362', 'CustomerId_320', 'CustomerId_313', 'CustomerId_496', 'CustomerId_67', 'CustomerId_437', 'CustomerId_277', 'CustomerId_58', 'CustomerId_368', 'CustomerId_129', 'CustomerId_283', 'CustomerId_423', 'CustomerId_295', 'CustomerId_55', 'CustomerId_473', 'CustomerId_361', 'CustomerId_415', 'CustomerId_343', 'CustomerId_356', 'CustomerId_200', 'CustomerId_83', 'CustomerId_280', 'CustomerId_372']\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "for col in ['ChannelId','ProviderId', 'InvestorId',\n",
    "       'SubscriptionId', 'ProductCategory', 'ProductId', 'CustomerId']:\n",
    "    train_col = train[col].unique()\n",
    "    test_col = test[col].unique()\n",
    "    \n",
    "    print ('Not in Test ' + col + ' ',[i  for i in train_col if i not in test_col])\n",
    "    print ('Not in Train ' + col + ' ',[i  for i in test_col  if i not in train_col])\n",
    "    print('==' * 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Embedding with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_FE(train, test, cols, normalize = True, ext_train = None, ext_test= None):\n",
    "    norm = normalize\n",
    "    for col in cols:\n",
    "        if ext_train is None:\n",
    "            df = pd.concat([train[col],test[col]])\n",
    "            nm = col+'_FE'\n",
    "        else:\n",
    "            df = pd.concat([ext_train[col],ext_test[col]])\n",
    "            nm = \"rejected\"+\"_\"+ col +\"_FE\" \n",
    "        vc = df.value_counts(dropna=True, normalize=norm).to_dict()\n",
    "        vc[-1] = -1\n",
    "        train[nm] = train[col].map(vc)\n",
    "        train[nm] = train[nm].astype('float32')\n",
    "        test[nm] = test[col].map(vc)\n",
    "        test[nm] = test[nm].astype('float32')\n",
    "        train[nm].fillna(0,inplace=True)\n",
    "        test[nm].fillna(0,inplace=True)\n",
    "                \n",
    "        del df; x=gc.collect()\n",
    "        print(nm,', ',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One_hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LABEL ENCODE\n",
    "def encode_LE(train,test,cols,verbose=True):\n",
    "    for col in cols:\n",
    "        df_comb = pd.concat([train[col],test[col]],axis=0)\n",
    "        df_comb,_ = df_comb.factorize(sort=True)\n",
    "        nm = col\n",
    "        if df_comb.max()>32000: \n",
    "            train[nm] = df_comb[:len(train)].astype('int32')\n",
    "            test[nm] = df_comb[len(train):].astype('int32')\n",
    "        else:\n",
    "            train[nm] = df_comb[:len(train)].astype('int16')\n",
    "            test[nm] = df_comb[len(train):].astype('int16')\n",
    "        del df_comb; x=gc.collect()\n",
    "        if verbose: print(nm,', ',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Advanced Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def encode_GB(group , columns, agg=['mean'], train_df=train, test_df=test, \n",
    "#               fillna=True, usena=False):\n",
    "#         # AGGREGATION OF values with \n",
    "\n",
    "#         group = group if type(group) is list else [group]\n",
    "#         columns = columns if type(columns) is list else[columns]\n",
    "#         agg = agg if type(agg) in [list, dict] else [agg]\n",
    "#         if type(agg) is not dict:\n",
    "#             agg = {a.__name__: a for a in agg}\n",
    "#         agg_encode_map = {}\n",
    "# #         replace_na = replace_na\n",
    "        \n",
    "        \n",
    "#         for column in columns:\n",
    "#             encode_df = df[group + [column]].groupby(group)[column].agg(list(agg.values()))\n",
    "#             encode_column_names = ['_'.join(group) + '_' + column + '_' + agg_name for agg_name in agg.keys()]\n",
    "#             encode_df.columns = encode_column_names\n",
    "#             agg_encode_map[column] = encode_df\n",
    "#             print(f'{column} fit processed {encode_df.shape}')\n",
    "#             result_df = df[group].set_index(group)\n",
    "            \n",
    "#             encode_df = agg_encode_map[column]\n",
    "#             for encode_col in encode_df.columns:\n",
    "#                 result_df[encode_col] = result_df.index.map(encode_df[encode_col].to_dict())\n",
    "#                 print(f'{column} transformed')\n",
    "#                 result_df = result_df.fillna(-1)\n",
    "#                 result_df.index = df.index\n",
    "#             return result_df\n",
    "\n",
    "def encode_AG__2(group ,main_columns, aggregations, train_df=train, test_df=test, ext_src=None,\n",
    "              fillna=True, usena=False):\n",
    "    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n",
    "    for main_column in main_columns:  \n",
    "            for agg_type in aggregations:\n",
    "                if ext_src is None: \n",
    "                    temp_df = pd.concat([train_df[group +[main_column]], test_df[group +[main_column]]])\n",
    "                    new_col_name = group[0]+\"_\"+group[1]+\"_\"+main_column+'_'+agg_type\n",
    "                                    \n",
    "                else:\n",
    "                    temp_df = ext_src.copy()\n",
    "                    new_col_name = \"ext_data\"+ \"_\"+group[0]+\"_\"+group[1]+\"_\"+main_column+'_'+agg_type\n",
    "                    \n",
    "                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n",
    "                temp_df = temp_df.groupby(group)[main_column].agg([agg_type]).reset_index(level=group).rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "                train_df[new_col_name] = pd.merge(train_df, temp_df, on=group, how='left')[new_col_name].astype('float32')\n",
    "                test_df[new_col_name]  = pd.merge(test_df, temp_df, on=group, how='left')[new_col_name].astype('float32')\n",
    "                \n",
    "                if fillna:\n",
    "                    train_df[new_col_name].fillna(-1,inplace=True)\n",
    "                    test_df[new_col_name].fillna(-1,inplace=True)\n",
    "                \n",
    "                print(\"'\"+new_col_name+\"'\",', ',end='')\n",
    "                \n",
    "# GROUP AGGREGATION MEAN AND STD\n",
    "# https://www.kaggle.com/kyakovlev/ieee-fe-with-some-eda\n",
    "def encode_AG(uids ,main_columns, aggregations, train_df=train, test_df=test, ext_src=None,\n",
    "              fillna=True, usena=False):\n",
    "    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            for agg_type in aggregations:\n",
    "                if ext_src is None: \n",
    "                    temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n",
    "                    new_col_name = main_column+'_'+col+'_'+agg_type\n",
    "                                    \n",
    "                else:\n",
    "                    temp_df = ext_src.copy()\n",
    "                    new_col_name = \"ext_data\"+ \"_\"+main_column+'_'+col+'_'+agg_type\n",
    "\n",
    "                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n",
    "                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                        columns={agg_type: new_col_name})\n",
    "\n",
    "                temp_df.index = list(temp_df[col])\n",
    "                temp_df = temp_df[new_col_name].to_dict()   \n",
    "\n",
    "                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n",
    "                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n",
    "                \n",
    "                if fillna:\n",
    "                    train_df[new_col_name].fillna(-1,inplace=True)\n",
    "                    test_df[new_col_name].fillna(-1,inplace=True)\n",
    "                \n",
    "                print(\"'\"+new_col_name+\"'\",', ',end='')\n",
    "                \n",
    "                \n",
    "# COMBINE FEATURES\n",
    "def encode_CB(col1,col2,df1=train,df2=test):\n",
    "    nm = col1+'_'+col2\n",
    "    df1[nm] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n",
    "    df2[nm] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n",
    "#     encode_LE(nm,verbose=False)\n",
    "    print(nm,', ',end='')\n",
    "    \n",
    "# GROUP AGGREGATION NUNIQUE\n",
    "def encode_AG2( uids,main_columns, train_df=train, test_df=test):\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n",
    "            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n",
    "            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n",
    "            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n",
    "            print(col+'_'+main_column+'_ct, ',end='')\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Enginneering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get  rejected loan and remove from the main dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## df rejected loan\n",
    "df_rejected = train[train.IsDefaulted.isnull()]\n",
    "## remove multiple, duplicate Transaction\n",
    "df_rejected.drop_duplicates(subset=['CustomerId','TransactionId'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "## Get number of rejected loan for each customer\n",
    "df_rejected_test = test[test.LoanId.isnull()]\n",
    "## remove multiple, duplicate Transaction\n",
    "df_rejected_test.drop_duplicates( subset=['CustomerId','TransactionId'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove rejected loan\n",
    "cleaned_test = test.iloc[test.IssuedDateLoan[~test.LoanId.isnull()].index]\n",
    "cleaned_train = train.iloc[train.IssuedDateLoan[~train.LoanId.isnull()].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rejected_CustomerId_FE , CustomerId_TransactionId_ct, "
     ]
    }
   ],
   "source": [
    "### Get count of defaulters rejected loan\n",
    "encode_FE(cleaned_train, cleaned_test, ['CustomerId'], normalize=False, ext_train=df_rejected, ext_test=df_rejected_test)\n",
    "## count of accepted loan\n",
    "encode_AG2(['CustomerId'] ,  ['TransactionId'], train_df=cleaned_train, test_df=cleaned_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# get loan ratio\n",
    "cleaned_train[\"rejected_loan_ratio\"] = cleaned_train.rejected_CustomerId_FE/(cleaned_train.rejected_CustomerId_FE + cleaned_train.CustomerId_TransactionId_ct)\n",
    "cleaned_test[\"rejected_loan_ratio\"] = cleaned_test.rejected_CustomerId_FE/(cleaned_test.rejected_CustomerId_FE + cleaned_test.CustomerId_TransactionId_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Value_CustomerId_mean' , 'Value_CustomerId_min' , 'Value_CustomerId_max' , 'Value_CustomerId_std' , CustomerId_ProductId_ct, CustomerId_ProductCategory_ct, "
     ]
    }
   ],
   "source": [
    "encode_AG(['CustomerId'] ,  ['Value'], ['mean','min','max','std'], train_df=cleaned_train, test_df=cleaned_test, \n",
    "              fillna=True, usena=False)\n",
    "# how many type of product this customers are used to\n",
    "encode_AG2(['CustomerId'] ,  ['ProductId', 'ProductCategory'], train_df=cleaned_train, test_df=cleaned_test)\n",
    "# encode_AG2(['CustomerId'] ,  ['Product'], train_df=cleaned_train, test_df=cleaned_test)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomerId_FE , TransactionId_FE , CustomerId_TransactionId_ct, "
     ]
    }
   ],
   "source": [
    "## Get mean/median number of times a customer pays a loan \n",
    "##Get count of customer\n",
    "encode_FE(cleaned_train, cleaned_test, ['CustomerId', 'TransactionId'], normalize=False)\n",
    "## Get unique number of transaction of customers\n",
    "encode_AG2(['CustomerId'] ,  ['TransactionId'], train_df=cleaned_train, test_df=cleaned_test)\n",
    "\n",
    "cleaned_train[\"meanTransactionPerLoan\"] = cleaned_train[\"CustomerId_FE\"]/cleaned_train[\"CustomerId_TransactionId_ct\"]\n",
    "cleaned_test[\"meanTransactionPerLoan\"] = cleaned_test[\"CustomerId_FE\"]/cleaned_test[\"CustomerId_TransactionId_ct\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide meanTransac by the current trans\n",
    "cleaned_train[\"Value_Mean_Ratio\"] = cleaned_train[\"Value\"]/cleaned_train[\"Value_CustomerId_mean\"]\n",
    "cleaned_test[\"Value_Mean_Ratio\"] = cleaned_test[\"Value\"]/cleaned_test[\"Value_CustomerId_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subtract meanTransac by the current trans\n",
    "cleaned_train[\"Value_Mean_Minus\"] = cleaned_train[\"Value\"]-cleaned_train[\"Value_CustomerId_mean\"]\n",
    "cleaned_test[\"Value_Mean_Minus\"] = cleaned_test[\"Value\"]-cleaned_test[\"Value_CustomerId_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProductId_ProductCategory , "
     ]
    }
   ],
   "source": [
    "##combine product and ca`btegorical\n",
    "encode_CB(\"ProductId\",\"ProductCategory\",df1=cleaned_train,df2=cleaned_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ext_data_Value_CustomerId_mean' , 'ext_data_Value_CustomerId_min' , 'ext_data_Value_CustomerId_max' , 'ext_data_Value_CustomerId_std' , "
     ]
    }
   ],
   "source": [
    "## Get max and min of customer expense per product using previous transaction data\n",
    "encode_AG(['CustomerId'] ,  ['Value'], ['mean','min','max','std'], train_df=cleaned_train, test_df=cleaned_test, \n",
    "              fillna=True, usena=False, ext_src=unlinked_masked_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'CustomerId_ProductId_Value_mean' , 'CustomerId_ProductId_Value_min' , 'CustomerId_ProductId_Value_max' , 'CustomerId_ProductId_Value_std' , 'CustomerId_ProductCategory_Value_mean' , 'CustomerId_ProductCategory_Value_min' , 'CustomerId_ProductCategory_Value_max' , 'CustomerId_ProductCategory_Value_std' , "
     ]
    }
   ],
   "source": [
    "## mean,max,min,std of cost of previous loan per productId\n",
    "encode_AG__2(['CustomerId',\"ProductId\"] ,  ['Value'], ['mean','min','max','std'], train_df=cleaned_train, test_df=cleaned_test, \n",
    "              fillna=True, usena=False)\n",
    "\n",
    "## mean,max,min,std of cost of previous loan per productCategory\n",
    "encode_AG__2(['CustomerId',\"ProductCategory\"] ,  ['Value'], ['mean','min','max','std'], train_df=cleaned_train, test_df=cleaned_test, \n",
    "              fillna=True, usena=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ext_data_CustomerId_ProductId_Value_mean' , 'ext_data_CustomerId_ProductId_Value_min' , 'ext_data_CustomerId_ProductId_Value_max' , 'ext_data_CustomerId_ProductId_Value_std' , 'ext_data_CustomerId_ProductCategory_Value_mean' , 'ext_data_CustomerId_ProductCategory_Value_min' , 'ext_data_CustomerId_ProductCategory_Value_max' , 'ext_data_CustomerId_ProductCategory_Value_std' , "
     ]
    }
   ],
   "source": [
    "## mean,max,min,std of cost of previous loan per productId\n",
    "encode_AG__2(['CustomerId',\"ProductId\"] ,  ['Value'], ['mean','min','max','std'], train_df=cleaned_train, test_df=cleaned_test, \n",
    "              fillna=True, usena=False, ext_src=unlinked_masked_final)\n",
    "\n",
    "## mean,max,min,std of cost of previous loan per productCategory\n",
    "encode_AG__2(['CustomerId',\"ProductCategory\"] ,  ['Value'], ['mean','min','max','std'], train_df=cleaned_train, test_df=cleaned_test, \n",
    "              fillna=True, usena=False, ext_src=unlinked_masked_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ratio of product_value mean to current value mean\n",
    "cleaned_train[\"CustomerId_ProductCategory_Value_Ratio\"] = cleaned_train[\"Value\"]/cleaned_train[\"CustomerId_ProductCategory_Value_mean\"]\n",
    "cleaned_test[\"CustomerId_ProductCategory_Value_Ratio\"] = cleaned_test[\"Value\"]/cleaned_test[\"CustomerId_ProductCategory_Value_mean\"]\n",
    "\n",
    "## Ratio of product_value mean to current value mean\n",
    "cleaned_train[\"CustomerId_ProductId_Value_Ratio\"] = cleaned_train[\"Value\"]/cleaned_train[\"CustomerId_ProductId_Value_mean\"]\n",
    "cleaned_test[\"CustomerId_ProductId_Value_Ratio\"] = cleaned_test[\"Value\"]/cleaned_test[\"CustomerId_ProductId_Value_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ratio of product_value mean to current value mean\n",
    "cleaned_train[\"ext_data_CustomerId_ProductCategory_Value_Ratio\"] = cleaned_train[\"Value\"]/cleaned_train[\"ext_data_CustomerId_ProductCategory_Value_mean\"]\n",
    "cleaned_test[\"ext_data_CustomerId_ProductCategory_Value_Ratio\"] = cleaned_test[\"Value\"]/cleaned_test[\"ext_data_CustomerId_ProductCategory_Value_mean\"]\n",
    "\n",
    "## Ratio of product_value mean to current value mean\n",
    "cleaned_train[\"ext_data_CustomerId_ProductId_Value_Ratio\"] = cleaned_train[\"Value\"]/cleaned_train[\"ext_data_CustomerId_ProductId_Value_mean\"]\n",
    "cleaned_test[\"ext_data_CustomerId_ProductId_Value_Ratio\"] = cleaned_test[\"Value\"]/cleaned_test[\"ext_data_CustomerId_ProductId_Value_mean\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicate transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train.drop_duplicates(subset=['CustomerId','TransactionId'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    1082\n",
       "1.0      71\n",
       "Name: IsDefaulted, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_train.IsDefaulted.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency Encode CustomerId and TransactionId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomerId_FE , TransactionId_FE , CustomerId , ProductCategory , ProductId , SubscriptionId , InvestorId , ProductId_ProductCategory , "
     ]
    }
   ],
   "source": [
    "#frequency encode\n",
    "encode_FE(cleaned_train, cleaned_test, ['CustomerId', 'TransactionId'])\n",
    "#Label encode customer Id\n",
    "encode_LE(cleaned_train, cleaned_test, ['CustomerId'])\n",
    "## label Encode\n",
    "encode_LE(cleaned_train, cleaned_test,['ProductCategory', 'ProductId','SubscriptionId','InvestorId',\"ProductId_ProductCategory\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove data greater than a value from the dataset        \n",
    "In order to do this, i would check my plot to make a better decision about this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "##LAbel Encode\n",
    "# Take log of amount to handle this issue\n",
    "# col = \"CustomerId\"\n",
    "# df = pd.concat([train[col],test[col]])\n",
    "# df.value_counts(dropna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_trans(train, cols):\n",
    "    for col in cols:\n",
    "        \n",
    "## Add features from date time\n",
    "        attr = ['Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n",
    "                'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start'] +['Hour', 'Minute', 'Second']\n",
    "        for n in attr: train[col + n] = getattr(train[col].dt, n.lower())\n",
    "        train[col + 'Elapsed'] = train[col].astype(np.int64) // 10 ** 9\n",
    "        \n",
    "def date_trans_due(train, cols):\n",
    "    for col in cols:\n",
    "        \n",
    "## Add features from date time\n",
    "        attr = ['Day', 'Dayofweek',\n",
    "                'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n",
    "        for n in attr: train[col + n] = getattr(train[col].dt, n.lower())\n",
    "        train[col + 'Elapsed'] = train[col].astype(np.int64) // 10 ** 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill missing dueDate\n",
    "cleaned_train.DueDate.fillna(cleaned_train['IssuedDateLoan'] +  pd.to_timedelta(30, unit='d'),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TransactionTime\n",
    "date_trans(cleaned_train, ['TransactionStartTime'])\n",
    "date_trans(cleaned_test, ['TransactionStartTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Issue Time\n",
    "cleaned_train['Issue_Trans_Diff'] = (cleaned_train.IssuedDateLoan - cleaned_train.TransactionStartTime).dt.total_seconds()\n",
    "cleaned_test['Issue_Trans_Diff'] = (cleaned_test.IssuedDateLoan - cleaned_test.TransactionStartTime).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all Due days are 30 Days, so calculate for test\n",
    "## Calculate due date in test since not in train\n",
    "## DueDate\n",
    "cleaned_test['DueDate'] = cleaned_test['IssuedDateLoan'] +  pd.to_timedelta(30, unit='d')\n",
    "date_trans_due(cleaned_train, ['DueDate'])\n",
    "date_trans_due(cleaned_test, ['DueDate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bool to Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train[cleaned_train.select_dtypes(include='bool').columns] = cleaned_train.select_dtypes(include='bool').astype(int)\n",
    "cleaned_test[cleaned_test.select_dtypes(include='bool').columns] = cleaned_test.select_dtypes(include='bool').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_start = ['Value', 'Amount', 'ProductId', 'ProductCategory', 'TransactionStatus',\n",
    "       'InvestorId', 'rejected_CustomerId_FE', 'CustomerId_TransactionId_ct',\n",
    "       'rejected_loan_ratio', 'Value_CustomerId_mean', 'Value_CustomerId_min',\n",
    "       'Value_CustomerId_max', 'Value_CustomerId_std',\n",
    "       'CustomerId_ProductId_ct', 'CustomerId_ProductCategory_ct',\n",
    "       'CustomerId_FE', 'TransactionId_FE', 'meanTransactionPerLoan',\n",
    "       'Value_Mean_Ratio', 'Value_Mean_Minus', 'ProductId_ProductCategory',\n",
    "       'ext_data_Value_CustomerId_mean', 'ext_data_Value_CustomerId_min',\n",
    "       'ext_data_Value_CustomerId_max', 'ext_data_Value_CustomerId_std',\n",
    "       'CustomerId_ProductId_Value_mean', 'CustomerId_ProductId_Value_min',\n",
    "       'CustomerId_ProductId_Value_max', 'CustomerId_ProductId_Value_std',\n",
    "       'CustomerId_ProductCategory_Value_mean',\n",
    "       'CustomerId_ProductCategory_Value_min',\n",
    "       'CustomerId_ProductCategory_Value_max',\n",
    "       'CustomerId_ProductCategory_Value_std',\n",
    "       'ext_data_CustomerId_ProductId_Value_mean',\n",
    "       'ext_data_CustomerId_ProductId_Value_min',\n",
    "       'ext_data_CustomerId_ProductId_Value_max',\n",
    "       'ext_data_CustomerId_ProductId_Value_std',\n",
    "       'ext_data_CustomerId_ProductCategory_Value_mean',\n",
    "       'ext_data_CustomerId_ProductCategory_Value_min',\n",
    "       'ext_data_CustomerId_ProductCategory_Value_max',\n",
    "       'ext_data_CustomerId_ProductCategory_Value_std',\n",
    "       'CustomerId_ProductCategory_Value_Ratio',\n",
    "       'CustomerId_ProductId_Value_Ratio',\n",
    "       'ext_data_CustomerId_ProductCategory_Value_Ratio',\n",
    "       'ext_data_CustomerId_ProductId_Value_Ratio',\n",
    "       'TransactionStartTimeMonth', 'TransactionStartTimeWeek',\n",
    "       'TransactionStartTimeDay', 'TransactionStartTimeDayofweek',\n",
    "       'TransactionStartTimeDayofyear', 'TransactionStartTimeIs_month_end',\n",
    "       'TransactionStartTimeIs_month_start',\n",
    "       'TransactionStartTimeIs_quarter_end',\n",
    "       'TransactionStartTimeIs_quarter_start',\n",
    "       'TransactionStartTimeIs_year_end', 'TransactionStartTimeIs_year_start',\n",
    "       'TransactionStartTimeHour', 'TransactionStartTimeMinute',\n",
    "       'TransactionStartTimeSecond', 'TransactionStartTimeElapsed',\n",
    "       'Issue_Trans_Diff', 'DueDateDay', 'DueDateDayofweek',\n",
    "       'DueDateIs_month_end', 'DueDateIs_month_start', 'DueDateIs_quarter_end',\n",
    "       'DueDateIs_quarter_start', 'DueDateIs_year_end', 'DueDateIs_year_start',\n",
    "       'DueDateElapsed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([cleaned_train, cleaned_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CustomerId', 'TransactionStartTime', 'Value', 'Amount',\n",
       "       'TransactionId', 'BatchId', 'SubscriptionId', 'CurrencyCode',\n",
       "       'CountryCode', 'ProviderId', 'ProductId', 'ProductCategory',\n",
       "       'ChannelId', 'TransactionStatus', 'IssuedDateLoan', 'LoanId',\n",
       "       'InvestorId', 'LoanApplicationId', 'ThirdPartyId',\n",
       "       'rejected_CustomerId_FE', 'CustomerId_TransactionId_ct',\n",
       "       'rejected_loan_ratio', 'Value_CustomerId_mean', 'Value_CustomerId_min',\n",
       "       'Value_CustomerId_max', 'Value_CustomerId_std',\n",
       "       'CustomerId_ProductId_ct', 'CustomerId_ProductCategory_ct',\n",
       "       'CustomerId_FE', 'TransactionId_FE', 'meanTransactionPerLoan',\n",
       "       'Value_Mean_Ratio', 'Value_Mean_Minus', 'ProductId_ProductCategory',\n",
       "       'ext_data_Value_CustomerId_mean', 'ext_data_Value_CustomerId_min',\n",
       "       'ext_data_Value_CustomerId_max', 'ext_data_Value_CustomerId_std',\n",
       "       'CustomerId_ProductId_Value_mean', 'CustomerId_ProductId_Value_min',\n",
       "       'CustomerId_ProductId_Value_max', 'CustomerId_ProductId_Value_std',\n",
       "       'CustomerId_ProductCategory_Value_mean',\n",
       "       'CustomerId_ProductCategory_Value_min',\n",
       "       'CustomerId_ProductCategory_Value_max',\n",
       "       'CustomerId_ProductCategory_Value_std',\n",
       "       'ext_data_CustomerId_ProductId_Value_mean',\n",
       "       'ext_data_CustomerId_ProductId_Value_min',\n",
       "       'ext_data_CustomerId_ProductId_Value_max',\n",
       "       'ext_data_CustomerId_ProductId_Value_std',\n",
       "       'ext_data_CustomerId_ProductCategory_Value_mean',\n",
       "       'ext_data_CustomerId_ProductCategory_Value_min',\n",
       "       'ext_data_CustomerId_ProductCategory_Value_max',\n",
       "       'ext_data_CustomerId_ProductCategory_Value_std',\n",
       "       'CustomerId_ProductCategory_Value_Ratio',\n",
       "       'CustomerId_ProductId_Value_Ratio',\n",
       "       'ext_data_CustomerId_ProductCategory_Value_Ratio',\n",
       "       'ext_data_CustomerId_ProductId_Value_Ratio',\n",
       "       'TransactionStartTimeMonth', 'TransactionStartTimeWeek',\n",
       "       'TransactionStartTimeDay', 'TransactionStartTimeDayofweek',\n",
       "       'TransactionStartTimeDayofyear', 'TransactionStartTimeIs_month_end',\n",
       "       'TransactionStartTimeIs_month_start',\n",
       "       'TransactionStartTimeIs_quarter_end',\n",
       "       'TransactionStartTimeIs_quarter_start',\n",
       "       'TransactionStartTimeIs_year_end', 'TransactionStartTimeIs_year_start',\n",
       "       'TransactionStartTimeHour', 'TransactionStartTimeMinute',\n",
       "       'TransactionStartTimeSecond', 'TransactionStartTimeElapsed',\n",
       "       'Issue_Trans_Diff', 'DueDate', 'DueDateDay', 'DueDateDayofweek',\n",
       "       'DueDateIs_month_end', 'DueDateIs_month_start', 'DueDateIs_quarter_end',\n",
       "       'DueDateIs_quarter_start', 'DueDateIs_year_end', 'DueDateIs_year_start',\n",
       "       'DueDateElapsed'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pca\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Create a scaler object\n",
    "sc = StandardScaler()\n",
    "# Fit the scaler to the features and transform\n",
    "X_std = sc.fit_transform(all_data[col_start])\n",
    "# Create a pca object with the 2 components as a parameter\n",
    "pca = decomposition.PCA(n_components=3)\n",
    "# Fit the PCA and transform the data\n",
    "X_std_pca = pca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data[\"pc1\"] = 0\n",
    "all_data[\"pc2\"] = 0\n",
    "all_data[\"pc3\"] = 0\n",
    "all_data[[\"pc1\",\"pc2\",\"pc3\"]] = X_std_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5, random_state=1996).fit(all_data[col_start])\n",
    "all_data[\"kmeans\"] = kmeans.predict(all_data[col_start])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1153, 91)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train = all_data[:1153]\n",
    "cleaned_test = all_data[1153:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get features to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_col = ['CustomerId', \n",
    "             'Value', \n",
    "#             'SubscriptionId', \n",
    "            'ProductId', \n",
    "            'ProductCategory',\n",
    "       \n",
    "#         'IsDefaulted', \n",
    "             'rejected_CustomerId_FE',\n",
    "       'CustomerId_TransactionId_ct',\n",
    "             'rejected_loan_ratio',\n",
    "       'Value_CustomerId_mean', 'Value_CustomerId_min', 'Value_CustomerId_max',\n",
    "       'Value_CustomerId_std', 'CustomerId_ProductId_ct',\n",
    "       'CustomerId_ProductCategory_ct', 'CustomerId_FE', 'TransactionId_FE',\n",
    "       'meanTransactionPerLoan', 'Value_Mean_Ratio', 'Value_Mean_Minus',\n",
    "       'ProductId_ProductCategory', 'ext_data_Value_CustomerId_mean',\n",
    "       'ext_data_Value_CustomerId_min', 'ext_data_Value_CustomerId_max',\n",
    "       'ext_data_Value_CustomerId_std', 'CustomerId_ProductId_Value_mean',\n",
    "       'CustomerId_ProductId_Value_min', 'CustomerId_ProductId_Value_max',\n",
    "       'CustomerId_ProductId_Value_std',\n",
    "       'CustomerId_ProductCategory_Value_mean',\n",
    "       'CustomerId_ProductCategory_Value_min',\n",
    "       'CustomerId_ProductCategory_Value_max',\n",
    "       'CustomerId_ProductCategory_Value_std',\n",
    "       'ext_data_CustomerId_ProductId_Value_mean',\n",
    "       'ext_data_CustomerId_ProductId_Value_min',\n",
    "       'ext_data_CustomerId_ProductId_Value_max',\n",
    "       'ext_data_CustomerId_ProductId_Value_std',\n",
    "       'ext_data_CustomerId_ProductCategory_Value_mean',\n",
    "       'ext_data_CustomerId_ProductCategory_Value_min',\n",
    "       'ext_data_CustomerId_ProductCategory_Value_max',\n",
    "       'ext_data_CustomerId_ProductCategory_Value_std',\n",
    "       'CustomerId_ProductCategory_Value_Ratio',\n",
    "       'CustomerId_ProductId_Value_Ratio',\n",
    "       'ext_data_CustomerId_ProductCategory_Value_Ratio',\n",
    "       'ext_data_CustomerId_ProductId_Value_Ratio',\n",
    "       'TransactionStartTimeMonth', 'TransactionStartTimeWeek',\n",
    "       'TransactionStartTimeDay', 'TransactionStartTimeDayofweek',\n",
    "       'TransactionStartTimeDayofyear', 'TransactionStartTimeIs_month_end',\n",
    "       'TransactionStartTimeIs_month_start',\n",
    "       'TransactionStartTimeIs_quarter_end',\n",
    "       'TransactionStartTimeIs_quarter_start',\n",
    "       'TransactionStartTimeHour', 'TransactionStartTimeMinute',\n",
    "       'TransactionStartTimeSecond', 'TransactionStartTimeElapsed',\n",
    "       'Issue_Trans_Diff', 'DueDateDay', 'DueDateDayofweek',\n",
    "       'DueDateIs_month_end', 'DueDateIs_month_start', 'DueDateIs_quarter_end',\n",
    "       'DueDateIs_quarter_start', 'DueDateIs_year_end', 'DueDateIs_year_start',\n",
    "       'DueDateElapsed','pc1', 'pc2', 'pc3', 'kmeans'\n",
    "            ]\n",
    "Target_name=\"IsDefaulted\"\n",
    "not_used_cols=[Target_name,'TransactionStartTime',\"TransactionStartTimeMonth\"]\n",
    "features_name=[ f for f in train_col if f not in not_used_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Scheme\n",
    "GroupKfold by months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are 6 group fold, so do a train test on the 6 fold\n",
    "from sklearn.model_selection import train_test_split, KFold, GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import  auc, roc_auc_score, roc_curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionId_FE                                      1\n",
       "DueDateIs_month_end                                   2\n",
       "DueDateIs_quarter_start                               2\n",
       "DueDateIs_quarter_end                                 2\n",
       "DueDateIs_month_start                                 2\n",
       "DueDateIs_year_start                                  2\n",
       "DueDateIs_year_end                                    2\n",
       "TransactionStartTimeIs_month_end                      2\n",
       "TransactionStartTimeIs_month_start                    2\n",
       "TransactionStartTimeIs_quarter_end                    2\n",
       "TransactionStartTimeIs_quarter_start                  2\n",
       "kmeans                                                3\n",
       "CustomerId_ProductCategory_ct                         6\n",
       "DueDateDayofweek                                      7\n",
       "ProductCategory                                       7\n",
       "CustomerId_ProductId_ct                               7\n",
       "TransactionStartTimeDayofweek                         7\n",
       "ProductId_ProductCategory                            15\n",
       "Issue_Trans_Diff                                     15\n",
       "ProductId                                            15\n",
       "rejected_CustomerId_FE                               16\n",
       "TransactionStartTimeWeek                             24\n",
       "TransactionStartTimeHour                             24\n",
       "ext_data_Value_CustomerId_min                        31\n",
       "TransactionStartTimeDay                              31\n",
       "DueDateDay                                           31\n",
       "CustomerId_TransactionId_ct                          31\n",
       "CustomerId_FE                                        31\n",
       "Value_CustomerId_min                                 34\n",
       "ext_data_CustomerId_ProductId_Value_min              38\n",
       "                                                   ... \n",
       "TransactionStartTimeMinute                           60\n",
       "ext_data_CustomerId_ProductCategory_Value_max        61\n",
       "rejected_loan_ratio                                  61\n",
       "ext_data_Value_CustomerId_max                        77\n",
       "Value                                                87\n",
       "Value_CustomerId_std                                128\n",
       "CustomerId_ProductId_Value_std                      128\n",
       "CustomerId_ProductCategory_Value_std                131\n",
       "Value_CustomerId_mean                               143\n",
       "CustomerId_ProductId_Value_mean                     145\n",
       "CustomerId_ProductCategory_Value_mean               148\n",
       "TransactionStartTimeDayofyear                       155\n",
       "ext_data_CustomerId_ProductCategory_Value_std       159\n",
       "ext_data_Value_CustomerId_std                       160\n",
       "ext_data_Value_CustomerId_mean                      171\n",
       "ext_data_CustomerId_ProductCategory_Value_mean      172\n",
       "ext_data_CustomerId_ProductId_Value_std             186\n",
       "ext_data_CustomerId_ProductId_Value_mean            196\n",
       "CustomerId                                          239\n",
       "Value_Mean_Minus                                    451\n",
       "Value_Mean_Ratio                                    455\n",
       "CustomerId_ProductId_Value_Ratio                    516\n",
       "CustomerId_ProductCategory_Value_Ratio              535\n",
       "ext_data_CustomerId_ProductCategory_Value_Ratio     537\n",
       "ext_data_CustomerId_ProductId_Value_Ratio           545\n",
       "DueDateElapsed                                     1149\n",
       "pc3                                                1153\n",
       "pc1                                                1153\n",
       "pc2                                                1153\n",
       "TransactionStartTimeElapsed                        1153\n",
       "Length: 69, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_train[features_name].nunique().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_gen =  GroupKFold(n_splits=6).split(cleaned_train, cleaned_train.IsDefaulted, groups=cleaned_train.TransactionStartTimeMonth)\n",
    "cv = list(cv_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_cat = [\n",
    "#         \"SubscriptionId\",\n",
    "#          \"TransactionStartTimeDayofweek\",\n",
    "#          \"ProductCategory\",\n",
    "         \"CustomerId\",\n",
    "#          \"ProductId\",\n",
    "#         \"TransactionStartTimeHour\",\n",
    "#     \"DueDateDay\"\n",
    "        ]   \n",
    "scale_feat = [ f for f in features_name if f not in to_cat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "train_col = [\n",
    "              'CustomerId_ProductId_ct',\n",
    "              'meanTransactionPerLoan',\n",
    "              'ext_data_Value_CustomerId_mean',\n",
    "              'ext_data_Value_CustomerId_max',\n",
    "              'ext_data_Value_CustomerId_std',\n",
    "              'ext_data_CustomerId_ProductCategory_Value_min',\n",
    "              'TransactionStartTimeWeek',\n",
    "              'Issue_Trans_Diff',\n",
    "              'DueDateIs_month_start',\n",
    "              'kmeans'\n",
    "            ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_col  =['rejected_loan_ratio',\n",
    "#               'CustomerId_ProductId_ct',\n",
    "#               'meanTransactionPerLoan',\n",
    "#               'ext_data_Value_CustomerId_max',\n",
    "#               'ext_data_Value_CustomerId_std',\n",
    "#               'ext_data_CustomerId_ProductCategory_Value_min',\n",
    "#               'CustomerId_ProductId_Value_Ratio',\n",
    "#               'TransactionStartTimeWeek',\n",
    "#               'TransactionStartTimeDayofweek',\n",
    "#               'TransactionStartTimeElapsed']\n",
    "\n",
    "\n",
    "# train_col =['CustomerId_ProductId_ct',\n",
    "#               'TransactionId_FE',\n",
    "#               'meanTransactionPerLoan',\n",
    "#               'ext_data_Value_CustomerId_mean',\n",
    "#               'ext_data_Value_CustomerId_max',\n",
    "#               'ext_data_Value_CustomerId_std',\n",
    "#               'ext_data_CustomerId_ProductCategory_Value_min',\n",
    "#               'TransactionStartTimeWeek',\n",
    "#               'TransactionStartTimeIs_quarter_end',\n",
    "#               'TransactionStartTimeIs_quarter_start',\n",
    "#               'Issue_Trans_Diff',\n",
    "#               'DueDateIs_month_start',\n",
    "#               'DueDateIs_quarter_end',\n",
    "#               'DueDateIs_quarter_start',\n",
    "#               'kmeans']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0\n",
      "0.9209401709401709\n",
      "Fold  1\n",
      "0.742013888888889\n",
      "Fold  2\n",
      "0.9411764705882353\n",
      "Fold  3\n",
      "0.9623798076923077\n",
      "Fold  4\n",
      "0.85062893081761\n",
      "Fold  5\n",
      "0.75\n",
      "['CustomerId_ProductId_ct', 'meanTransactionPerLoan', 'ext_data_Value_CustomerId_mean', 'ext_data_Value_CustomerId_max', 'ext_data_Value_CustomerId_std', 'ext_data_CustomerId_ProductCategory_Value_min', 'TransactionStartTimeWeek', 'Issue_Trans_Diff', 'DueDateIs_month_start', 'kmeans']\n",
      "0.9045846241961938\n"
     ]
    }
   ],
   "source": [
    "oof = np.zeros(len(cleaned_train))\n",
    "predictions = np.zeros(len(cleaned_test))\n",
    "\n",
    "Target_name=\"IsDefaulted\"\n",
    "not_used_cols=[Target_name,'TransactionStartTime']\n",
    "features_name=[ f for f in train_col if f not in not_used_cols]\n",
    "skf = GroupKFold(n_splits=6)\n",
    "for i, (idxT, idxV) in enumerate( skf.split(cleaned_train, cleaned_train.IsDefaulted, groups=cleaned_train.TransactionStartTimeMonth) ):\n",
    "    ## one hot encode\n",
    "#     all_data = pd.concat([cleaned_train[features_name], cleaned_test[features_name]])\n",
    "#     all_data[to_cat] = all_data[to_cat].astype(\"category\")\n",
    "#     all_data = pd.get_dummies(all_data,columns=to_cat)\n",
    "#     clean_train= all_data[:len(cleaned_train)]\n",
    "#     clean_test= all_data[len(cleaned_train):]\n",
    "    \n",
    "    \n",
    "    log_train = cleaned_train.loc[idxT,features_name]\n",
    "    log_test = cleaned_train.loc[idxV,features_name]\n",
    "\n",
    "#     log_train = clean_train.loc[idxT]\n",
    "#     log_test = clean_train.loc[idxV]\n",
    "    #### Take log of \n",
    "#     log_train.Value = np.log1p(log_train.Value)\n",
    "#     log_test.Value = np.log1p(log_test.Value)\n",
    "\n",
    "    log_train_y = cleaned_train['IsDefaulted'][idxT]\n",
    "    log_test_y = cleaned_train['IsDefaulted'][idxV]\n",
    "\n",
    "    \n",
    "#     log_train[scale_feat] = sc.fit_transform(log_train[scale_feat])\n",
    "#     log_test[scale_feat]= sc.fit_transform(log_test[scale_feat])\n",
    "\n",
    "\n",
    "    # Predict\n",
    "#     lr = LogisticRegression()\n",
    "    lr = lgb.LGBMClassifier( min_child_samples=60,\n",
    "                            num_leaves=20,\n",
    "                            min_child_weight=10,\n",
    "                                n_estimators=80,\n",
    "                                max_depth= 2, \n",
    "                                learning_rate=0.02, \n",
    "                                subsample=0.9,\n",
    "                            #     colsample_bytree=0.4, \n",
    "                                eval_metric='auc',\n",
    "                                class_weight=\"balanced\",\n",
    "                                # USE CPU\n",
    "                                nthread=4)\n",
    "    lr.fit(log_train,log_train_y)\n",
    "\n",
    "    pred = lr.predict_proba(log_test)[:,1]\n",
    "    oof[idxV] = pred\n",
    "    predictions += lr.predict_proba(cleaned_test[features_name])[:,1]/ skf.n_splits\n",
    "    print('Fold ', i)\n",
    "    print (roc_auc_score(log_test_y, pred))\n",
    "print(features_name)\n",
    "print(roc_auc_score(cleaned_train['IsDefaulted'], oof))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  0\n",
      "0.8811780007432181\n",
      "Fold  1\n",
      "0.834375\n",
      "Fold  2\n",
      "0.8760504201680672\n",
      "Fold  3\n",
      "0.9691105769230769\n",
      "Fold  4\n",
      "0.820754716981132\n",
      "Fold  5\n",
      "0.7857142857142857\n",
      "['CustomerId_ProductId_ct', 'meanTransactionPerLoan', 'ext_data_Value_CustomerId_mean', 'ext_data_Value_CustomerId_max', 'ext_data_Value_CustomerId_std', 'ext_data_CustomerId_ProductCategory_Value_min', 'TransactionStartTimeWeek', 'Issue_Trans_Diff', 'DueDateIs_month_start', 'kmeans']\n",
      "0.891971049959647\n"
     ]
    }
   ],
   "source": [
    "oof = np.zeros(len(cleaned_train))\n",
    "Target_name=\"IsDefaulted\"\n",
    "not_used_cols=[Target_name,'TransactionStartTime']\n",
    "features_name=[ f for f in train_col if f not in not_used_cols]\n",
    "skf = GroupKFold(n_splits=6)\n",
    "for i, (idxT, idxV) in enumerate( skf.split(cleaned_train, cleaned_train.IsDefaulted, groups=cleaned_train.TransactionStartTimeMonth) ):\n",
    "   \n",
    "    log_train = cleaned_train.loc[idxT,features_name]\n",
    "    log_test = cleaned_train.loc[idxV,features_name]\n",
    "    \n",
    "    #target value\n",
    "    log_train_y = cleaned_train['IsDefaulted'][idxT]\n",
    "    log_test_y = cleaned_train['IsDefaulted'][idxV]\n",
    "\n",
    "    # fit model\n",
    "\n",
    "    rf = RandomForestClassifier(random_state=2001, n_estimators=2000, max_depth=2,class_weight='balanced',min_samples_split=100,min_samples_leaf=90)\n",
    "    rf.fit(log_train,log_train_y)\n",
    "\n",
    "    pred = rf.predict_proba(log_test)[:,1]\n",
    "    oof[idxV] = pred\n",
    "    print('Fold ', i)\n",
    "    print (roc_auc_score(log_test_y, pred))\n",
    "print(features_name)\n",
    "print(roc_auc_score(cleaned_train['IsDefaulted'], oof))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit on all train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=2, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=90,\n",
       "            min_samples_split=100, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=2000, n_jobs=None, oob_score=False,\n",
       "            random_state=2001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(cleaned_train[features_name], cleaned_train.IsDefaulted)\n",
    "rf.fit(cleaned_train[features_name], cleaned_train.IsDefaulted)\n",
    "# lr.fit(sc.fit_transform(cleaned_train[features_name]), cleaned_train.IsDefaulted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lgb prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['IsDefaulted'] = np.nan\n",
    "test.loc[cleaned_test.index, 'IsDefaulted']= lr.predict_proba(cleaned_test[features_name])[:,1]\n",
    "pd.DataFrame({\"TransactionId\": test.TransactionId.values, 'IsDefaulted':test.IsDefaulted}).to_csv(\"lgb_new_values.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['IsDefaulted'] = np.nan\n",
    "test.loc[cleaned_test.index, 'IsDefaulted']= rf.predict_proba(cleaned_test[features_name])[:,1]\n",
    "pd.DataFrame({\"TransactionId\": test.TransactionId.values, 'IsDefaulted':test.IsDefaulted}).to_csv(\"rf_new_values.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CustomerId_ProductId_ct',\n",
       " 'meanTransactionPerLoan',\n",
       " 'ext_data_Value_CustomerId_mean',\n",
       " 'ext_data_Value_CustomerId_max',\n",
       " 'ext_data_Value_CustomerId_std',\n",
       " 'ext_data_CustomerId_ProductCategory_Value_min',\n",
       " 'TransactionStartTimeWeek',\n",
       " 'Issue_Trans_Diff',\n",
       " 'DueDateIs_month_start',\n",
       " 'kmeans']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.101027\n",
       "1      0.189407\n",
       "2      0.141722\n",
       "3      0.141983\n",
       "4      0.141983\n",
       "5           NaN\n",
       "6           NaN\n",
       "7      0.144534\n",
       "8           NaN\n",
       "9           NaN\n",
       "10     0.312412\n",
       "11     0.153006\n",
       "12     0.099766\n",
       "13     0.099766\n",
       "14     0.583180\n",
       "15     0.333153\n",
       "16     0.113974\n",
       "17     0.085639\n",
       "18     0.086145\n",
       "19          NaN\n",
       "20     0.206165\n",
       "21     0.583180\n",
       "22     0.085639\n",
       "23     0.333153\n",
       "24     0.583180\n",
       "25     0.477846\n",
       "26     0.085639\n",
       "27          NaN\n",
       "28          NaN\n",
       "29     0.167195\n",
       "         ...   \n",
       "875         NaN\n",
       "876         NaN\n",
       "877         NaN\n",
       "878         NaN\n",
       "879    0.218115\n",
       "880    0.177785\n",
       "881         NaN\n",
       "882         NaN\n",
       "883    0.214942\n",
       "884         NaN\n",
       "885         NaN\n",
       "886         NaN\n",
       "887         NaN\n",
       "888         NaN\n",
       "889         NaN\n",
       "890         NaN\n",
       "891         NaN\n",
       "892         NaN\n",
       "893         NaN\n",
       "894    0.162520\n",
       "895         NaN\n",
       "896         NaN\n",
       "897         NaN\n",
       "898    0.822098\n",
       "899         NaN\n",
       "900         NaN\n",
       "901         NaN\n",
       "902         NaN\n",
       "903         NaN\n",
       "904         NaN\n",
       "Name: IsDefaulted, Length: 905, dtype: float64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.IsDefaulted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = 0.3*pd.read_csv('lgb_new_values.csv')['IsDefaulted'] +  0.7*pd.read_csv('rf_new_values.csv')['IsDefaulted'] \n",
    "pd.DataFrame({\"TransactionId\": test.TransactionId.values, 'IsDefaulted':avg}).to_csv(\"RanF_LGB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My high score was as a result of handling overfitting through handling of overfitting features(max_depth, min_sample_split,num_leaves...) . This was done because the model was highly overfitting the trainset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "653.333px",
    "left": "754px",
    "top": "109.437px",
    "width": "364.25px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
